# Open Source LLMs

Table of Contents

- [Pretrained Model](#pretrained-model)
- [Multitask Supervised Finetuned Model](#multitask-supervised-finetuned-model)
- [Instruction Finetuned Model](#instruction-finetuned-model)
  - [English](#english)
  - [Chinese](#chinese)
  - [Multilingual](#multilingual)
- [Human Feedback Finetuned Model](#human-feedback-finetuned-models)
- [Domain Finetuned Model](#domain-finetuned-model)
- [Open Source Projects](#open-source-projects)
  - [reproduce/framework](#reproduceframework)
  - [accelerate](#accelerate)
  - [evaluation](#evaluation)
  - [deployment/demo](#deploymentdemo)
- [Reference](#reference)

notes:

1. *Date: YY/MM, All projects are sorted by first proposed date*
2. *Dec=Decoder; Enc=Encoder; CTX=Context Length; WD=Window Length*
3. *We provide Huggingface (HF) checkpoint by default*

## Pretrained Model

| Model                        | Available Size                                                                                                                                                                                                                                                                                                                                                                                                                               | CTX   | Architecture | Training Data                                                                                                                                                                                     | Link                                                                                                                                                                                                      | Date  | Affiliation  |
| ---------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- | ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- | ------------ |
| Yi                           | [6B](https://huggingface.co/01-ai/Yi-6B)/[34B](https://huggingface.co/01-ai/Yi-34B)                                                                                                                                                                                                                                                                                                                                                                | 4K WD | Dec          | ?                                                                                                                                                                                                 | [code](https://github.com/01-ai/Yi)                                                                                                                                                                          | 11/23 | 01AI         |
| Skywork                      | [7B](https://huggingface.co/Skywork/Skywork-13B-Base)/[13B](https://huggingface.co/datasets/Skywork/SkyPile-150B)                                                                                                                                                                                                                                                                                                                                  | 4K    | Dec          | self-construct (500B/2TB/3.1TB tokens, Chinese&English)                                                                                                                                          | [code](https://github.com/SkyworkAI/Skywork)                                                                                                                                                                 | 10/23 | SkyworkAI    |
| Mistral                      | [7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)                                                                                                                                                                                                                                                                                                                                                                                          | 4K WD | Dec          | ?                                                                                                                                                                                                 | [code](https://github.com/mistralai/mistral-src), [blog](https://mistral.ai/news/announcing-mistral-7b/)                                                                                                        | 09/23 | Mistral AI   |
| XVERSE                       | [13B](https://huggingface.co/xverse/XVERSE-13B)                                                                                                                                                                                                                                                                                                                                                                                                 | 8K    | Dec          | self-construct (1.4T tokens, Multilingual)                                                                                                                                                        | [code](https://github.com/xverse-ai/XVERSE-13B)                                                                                                                                                              | 08/23 | xverse.ai    |
| Qwen-7B                      | [7B](https://huggingface.co/Qwen/Qwen-7B)                                                                                                                                                                                                                                                                                                                                                                                                       | 2K    | Dec          | self-construct (2.2T tokens)                                                                                                                                                                      | [code](https://github.com/QwenLM/Qwen-7B)                                                                                                                                                                    | 08/23 | Alibaba      |
| Llama 2 `Commercial`       | [7B](https://huggingface.co/meta-llama/Llama-2-7b), [13B](https://huggingface.co/meta-llama/Llama-2-13b), [70B](https://huggingface.co/meta-llama/Llama-2-70b)                                                                                                                                                                                                                                                                                        | 4K    | Dec          | self-construct (2T tokens)                                                                                                                                                                        | [code](https://github.com/facebookresearch/llama), [blog](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)                                                        | 07/23 | Meta         |
| PolyLM                       | [1.7B](https://huggingface.co/DAMO-NLP-MT/polylm-1.7b), [13B](https://huggingface.co/DAMO-NLP-MT/polylm-7b)                                                                                                                                                                                                                                                                                                                                        | 2K    | Dec          | self-construct (PT + INST, Multilingual)                                                                                                                                                          | [blog](https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation/summary)                                                                                                                             | 07/23 | Alibaba      |
| InternLM                     | [7B](https://huggingface.co/internlm/internlm-7b),[Original](https://github.com/InternLM/InternLM#model-zoo)                                                                                                                                                                                                                                                                                                                                       | 8K    | Dec          | self-construct (1.6T)                                                                                                                                                                             | [code](https://github.com/InternLM/InternLM)                                                                                                                                                                 | 07/23 | SenseTime    |
| 天鹰 (Aquila)                | 7B/33B,[Original](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila)                                                                                                                                                                                                                                                                                                                                                            | 2K    | Dec          | ? (English & Chinese)                                                                                                                                                                             | [code](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila)                                                                                                                                    | 06/23 | BAAI         |
| XGen                         | [7B-4K](https://huggingface.co/Salesforce/xgen-7b-4k-base)/[7B-8K]()                                                                                                                                                                                                                                                                                                                                                                               | 4K/8K | Dec          | [manual-mixture](https://blog.salesforceairesearch.com/xgen/) (1.2T/1.5T tokens, Multilingual)                                                                                                      | [code](https://github.com/salesforce/xgen), [blog](https://blog.salesforceairesearch.com/xgen/)                                                                                                                | 06/23 | Salesforce   |
| baichuan<br />`Commercial` | [7B](https://huggingface.co/baichuan-inc/baichuan-7B)                                                                                                                                                                                                                                                                                                                                                                                           | 4K    | Dec          | self-construct (1.2T tokens, English&Chinese)                                                                                                                                                     | [code](https://github.com/baichuan-inc/baichuan-7B)                                                                                                                                                          | 06/23 | Baichuan     |
| Tigerbot                     | [7B](https://huggingface.co/TigerResearch/tigerbot-7b-base)                                                                                                                                                                                                                                                                                                                                                                                     | 2K    | Dec          | [self-construct](https://github.com/TigerResearch/TigerBot#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE) (100GB,  web, book, English&Chinese)                                                     | [code](https://github.com/TigerResearch/TigerBot)                                                                                                                                                            | 06/23 | Tigerobo     |
| Falcon                       | [7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b)                                                                                                                                                                                                                                                                                                                                                      | 2K    | Dec          | [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) (600B tokens, English, web)                                                                                                  | [blog](https://falconllm.tii.ae/)                                                                                                                                                                            | 05/23 | TII          |
| StarCoder `code`           | [15.5B](https://huggingface.co/bigcode/starcoderbase)                                                                                                                                                                                                                                                                                                                                                                                           | 8K    | Dec          | [The Stack](https://huggingface.co/datasets/bigcode/the-stack-dedup) (1TB code)                                                                                                                     | [paper](https://arxiv.org/abs/2305.06161), [code](https://github.com/bigcode-project/starcoder/tree/main)                                                                                                       | 05/23 | Huggingface  |
| CodeGen2 `code`            | [1B](https://huggingface.co/Salesforce/codegen2-1B)/[3.7B](https://huggingface.co/Salesforce/codegen2-3_7B)/[7B](https://huggingface.co/Salesforce/codegen2-7B)/[16B](https://huggingface.co/Salesforce/codegen2-16B)                                                                                                                                                                                                                                    | 2K    | Dec          | [The Stack](https://huggingface.co/datasets/bigcode/the-stack-dedup) (3TB code)                                                                                                                     | [paper](https://arxiv.org/abs/2305.02309), [code](https://github.com/salesforce/CodeGen), [blog](https://huggingface.co/blog/starcoder)                                                                            | 05/23 | Salesforce   |
| StableLM-Alpha               | [3B](https://huggingface.co/stabilityai/stablelm-base-alpha-3b)/[7B](https://huggingface.co/stabilityai/stablelm-base-alpha-7b),[Original](https://github.com/Stability-AI/StableLM#stablelm-alpha)                                                                                                                                                                                                                                                   | 4K    | Dec          | self-construct (1.2T tokens, English)                                                                                                                                                             | [code](https://github.com/Stability-AI/StableLM#stablelm-alpha)                                                                                                                                              | 04/23 | Stability.AI |
| Pythia                       | [1B](https://huggingface.co/EleutherAI/pythia-1b)/[1.4B](https://huggingface.co/EleutherAI/pythia-1.4b)/[2.8B](https://huggingface.co/EleutherAI/pythia-2.8b)/[6.9B](https://huggingface.co/EleutherAI/pythia-6.9b)/[12B](https://huggingface.co/EleutherAI/pythia-12b),[Original](https://github.com/EleutherAI/pythia#models)                                                                                                                                | 2K    | Dec          | [The Pile](https://pile.eleuther.ai/) (400B tokens, English)                                                                                                                                        | [paper](https://arxiv.org/abs/2304.01373), [code](https://github.com/EleutherAI/pythia), [blog](https://www.eleuther.ai/papers-blog/pythia-a-suite-for-analyzing-large-language-modelsacross-training-and-scaling) | 04/23 | EleutherAI   |
| MPT                          | [7B](https://huggingface.co/mosaicml/mpt-7b)/[30B](https://huggingface.co/mosaicml/mpt-30b)                                                                                                                                                                                                                                                                                                                                                        | 8K    | Dec          | [C4](https://huggingface.co/datasets/allenai/c4), [The Stack](https://huggingface.co/datasets/bigcode/the-stack-dedup), [MC4](https://huggingface.co/datasets/mc4) (1T tokens, text + code, Multilingual) | [code](https://github.com/mosaicml/llm-foundry#mpt), [blog](https://www.mosaicml.com/blog/mpt-7b)                                                                                                               | 04/23 | MosaicML     |
| Cerebras-GPT                 | [1.3B](https://huggingface.co/cerebras/Cerebras-GPT-1.3B)/[2.7B](https://huggingface.co/cerebras/Cerebras-GPT-2.7B)/[6.7B](https://huggingface.co/cerebras/Cerebras-GPT-6.7B)/[13B](https://huggingface.co/cerebras/Cerebras-GPT-13B)                                                                                                                                                                                                                    | 2K    | Dec          | [The Pile](https://pile.eleuther.ai/) (400B tokens, English)                                                                                                                                        | [paper](https://arxiv.org/abs/2304.03208),[code](https://github.com/Cerebras/modelzoo), [blog](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/)               | 03/23 | Cerebras     |
| LLaMA                        | [7B](https://huggingface.co/huggyllama/llama-7b)/[13B](https://huggingface.co/huggyllama/llama-13b)/[33B](https://huggingface.co/huggyllama/llama-30b)/[65B](https://huggingface.co/huggyllama/llama-65b), [Request](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform) or [OpenLLaMA](https://github.com/openlm-research/open_llama)                                                                       | 2K    | Dec          | manual-mixture (1.4T tokens) or[RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data)                                                                                                | [paper](https://arxiv.org/abs/2302.13971), [code](https://github.com/facebookresearch/llama), [blog](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)                                             | 02/23 | Meta         |
| Galactica                    | [1.3B](https://huggingface.co/facebook/galactica-1.3b)/[6.7B](https://huggingface.co/facebook/galactica-6.7b)/[30B](https://huggingface.co/facebook/galactica-30b)/[120B](https://huggingface.co/facebook/galactica-120b)                                                                                                                                                                                                                                | 2K    | Dec          | scientific text and data (106B tokens)                                                                                                                                                            | [paper](https://arxiv.org/abs/2211.09085), [blog](https://galactica.org/)                                                                                                                                       | 11/22 | Meta         |
| BLOOM                        | [1.1B](https://huggingface.co/bigscience/bloom-1b1)/[1.7B](https://huggingface.co/bigscience/bloom-1b1)/[3B](https://huggingface.co/bigscience/bloom-3b)/[7.1B](https://huggingface.co/bigscience/bloom-7b1)                                                                                                                                                                                                                                             | 2K    | Dec          | [ROOTS](https://huggingface.co/spaces/bigscience-data/roots-search) (388B tokens, Multilingual, HF datasets)                                                                                        | [paper](https://arxiv.org/abs/2211.05100)                                                                                                                                                                    | 11/22 | BigScience   |
| RWKV-v4                      | [1.4B](https://huggingface.co/BlinkDL/rwkv-4-pile-1b5)/[3B](https://huggingface.co/BlinkDL/rwkv-4-pile-3b)/[7B](https://huggingface.co/BlinkDL/rwkv-4-pile-7b)/[14B](https://huggingface.co/BlinkDL/rwkv-4-pile-14b)                                                                                                                                                                                                                                     | -     | RWKV         | [The Pile](https://pile.eleuther.ai/) (400B tokens, English)                                                                                                                                        | [paper](https://arxiv.org/abs/2305.13048), [code](https://github.com/BlinkDL/RWKV-LM)                                                                                                                           | 09/22 | BlinkDL      |
| YALM                         | [100B](https://huggingface.co/yandex/yalm-100b/tree/main)                                                                                                                                                                                                                                                                                                                                                                                       | 1K    | Dec          | [The Pile](https://pile.eleuther.ai/), [Texts in Russian]() (300B tokens, English + Russian)                                                                                                           | [code](https://github.com/yandex/YaLM-100B), [blog](https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6)                                   | 06/22 | Yandex       |
| UL2                          | [20B](https://huggingface.co/google/ul2#:~:text=UL2%20is%20a%20unified%20framework%20for%20pretraining%20models,downstream%20fine-tuning%20is%20associated%20with%20specific%20pre-training%20schemes.)                                                                                                                                                                                                                                         | 0.5K  | Enc-Dec/Dec  | [C4](https://www.tensorflow.org/datasets/catalog/c4) (365B tokens, English, web)                                                                                                                    | [paper](https://arxiv.org/abs/2205.05131v1),[blog](https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html)                                                                                 | 05/22 | Google       |
| OPT                          | [1.3B](https://huggingface.co/facebook/opt-1.3b)/[2.7B](https://huggingface.co/facebook/opt-2.7b)/[6.7B](https://huggingface.co/facebook/opt-6.7b)/[13B](https://huggingface.co/facebook/opt-13b)/[30B](https://huggingface.co/facebook/opt-30b)/[66B](https://huggingface.co/facebook/opt-66b)/[175B (request)](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT), [Original](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT) | 2K    | Dec          | [The Pile](https://pile.eleuther.ai/), [PushShift.io Reddit](https://zenodo.org/record/3608135), [Roberta](https://github.com/facebookresearch/fairseq/issues/2947) (180B tokens, mainly English)         | [paper](https://arxiv.org/abs/2205.01068), [code](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT)                                                                                          | 05/22 | Meta         |
| GPT-NeoX                     | [20B-Original](https://github.com/EleutherAI/gpt-neox#gpt-neox-20b)                                                                                                                                                                                                                                                                                                                                                                             | 2K    | Dec          | [The Pile](https://pile.eleuther.ai/) (400B tokens, English)                                                                                                                                        | [code](https://github.com/EleutherAI/gpt-neox), [paper](https://arxiv.org/abs/2204.06745)                                                                                                                       | 04/22 | EleutherAI   |
| GPT-J                        | [6B](https://huggingface.co/EleutherAI/gpt-j-6b), [Original](https://github.com/kingoflolz/mesh-transformer-jax/#links)                                                                                                                                                                                                                                                                                                                           | 2K    | Dec          | [The Pile](https://pile.eleuther.ai/) (400B tokens, English)                                                                                                                                        | [code](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b)                                                                                                                                         | 05/21 | EleutherAI   |
| GPT-Neo                      | [1.3B](https://huggingface.co/EleutherAI/gpt-neo-1.3B)/[2.7B](https://huggingface.co/EleutherAI/gpt-neo-2.7B), [Original](https://github.com/EleutherAI/gpt-neo#pretrained-models)                                                                                                                                                                                                                                                                    | 2K    | Dec          | [The Pile](https://pile.eleuther.ai/) (400B tokens, English)                                                                                                                                        | [code](https://www.eleuther.ai/artifacts/gpt-neo), [blog](https://www.eleuther.ai/artifacts/gpt-neo)                                                                                                           | 03/21 | EleutherAI   |
| GLM                          | [2B](https://huggingface.co/THUDM/glm-2b)/[10B](https://huggingface.co/THUDM/glm-10b)/[10B-CHN](https://huggingface.co/THUDM/glm-10b-chinese), [Original](https://github.com/THUDM/GLM#pretrained-models)                                                                                                                                                                                                                                                | 1K    | Enc-Dec      | [The Pile](https://pile.eleuther.ai/) (400B tokens, English) or [WuDaoCorpora](https://data.baai.ac.cn/details/WuDaoCorporaText) (1080B tokens, Chinese)                                             | [paper](https://arxiv.org/abs/2103.10360), [code](https://github.com/THUDM/GLM)                                                                                                                                | 03/21 | THU          |
| mT5                          | [1.2B](https://huggingface.co/google/mt5-large)/[3.7B](https://huggingface.co/google/mt5-xl)/[13B](https://huggingface.co/google/mt5-xxl),[Original](https://github.com/google-research/multilingual-t5#released-model-checkpoints)                                                                                                                                                                                                                      | 0.5K  | Enc-Dec      | [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) (1T tokens, Multilingual, web)                                                                                                 | [paper](https://aclanthology.org/2021.naacl-main.41/), [code](https://github.com/google-research/multilingual-t5)                                                                                               | 10/20 | Google       |
| T5                           | [3B](https://huggingface.co/t5-3b)/[11B](https://huggingface.co/t5-11b), [Original](https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints)                                                                                                                                                                                                                                                                  | 0.5K  | Enc-Dec      | [C4](https://www.tensorflow.org/datasets/catalog/c4) (365B tokens, English, web)                                                                                                                   | [paper](https://jmlr.org/papers/v21/20-074.html), [code](https://github.com/google-research/text-to-text-transfer-transformer)                                                                                  | 10/19 | Google       |

## Multitask Supervised Finetuned Model

| Model     | Available Size                                                                                                                                                                                       | CTX  | Architecture | Base Model | Tuning Data                                                                             | Link                                                                                       | Date  | Affiliation |
| --------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | ------------ | ---------- | --------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ----- | ----------- |
| Bloomz    | [1.1B](https://huggingface.co/bigscience/bloomz-1b1)/[1.7B](https://huggingface.co/bigscience/bloomz-1b1)/[3B](https://huggingface.co/bigscience/bloomz-3b)/[7.1B](https://huggingface.co/bigscience/bloomz-7b1) | 2K   | Dec          | Bloom      | [xP3](https://huggingface.co/datasets/bigscience/xP3) (Multitask, Multilingual)           | [paper](https://arxiv.org/abs/2211.01786), [code](https://github.com/bigscience-workshop/xmtf)  | 11/22 | Bigscience  |
| Bloomz-mt | [7.1B](https://huggingface.co/bigscience/bloomz-7b1-mt)                                                                                                                                                 | 2K   | Dec          | Bloom      | [xP3mt](https://huggingface.co/datasets/bigscience/xP3mt) (Multitask, Multilingual+nonEN) | [paper](https://arxiv.org/abs/2211.01786), [code](https://github.com/bigscience-workshop/xmtf)  | 11/22 | Bigscience  |
| Bloomz-p3 | [7.1B](https://huggingface.co/bigscience/bloomz-7b1-p3)                                                                                                                                                 | 2K   | Dec          | Bloom      | [P3](https://huggingface.co/datasets/bigscience/P3) (Multitask)                           | [paper](https://arxiv.org/abs/2211.01786), [code](https://github.com/bigscience-workshop/xmtf)  | 11/22 | Bigscience  |
| mT0       | [1.2B](https://huggingface.co/bigscience/mt0-large)/[3.7B](https://huggingface.co/bigscience/mt0-xl)/[13B](https://huggingface.co/bigscience/mt0-xll)                                                         | 0.5K | Enc-Dec      | mT5        | [xP3mt](https://huggingface.co/datasets/bigscience/xP3mt) (Multitask, Multilingual+nonEN) | [paper](https://arxiv.org/abs/2211.01786), [code](https://github.com/bigscience-workshop/xmtf)  | 11/22 | Bigscience  |
| mT0-mt    | [13B](https://huggingface.co/bigscience/mt0-xxl-mt)                                                                                                                                                     | 0.5K | Enc-Dec      | mT5        | [xP3](https://huggingface.co/datasets/bigscience/xP3) (Multitask, Multilingual)           | [paper](https://arxiv.org/abs/2211.01786), [code](https://github.com/bigscience-workshop/xmtf)  | 11/22 | Bigscience  |
| mT0-p3    | [13B](https://huggingface.co/bigscience/mt0-xll-p3)                                                                                                                                                     | 0.5K | Enc-Dec      | mT5        | [P3](https://huggingface.co/datasets/bigscience/P3) (Multitask)                           | [paper](https://arxiv.org/abs/2211.01786), [code](https://github.com/bigscience-workshop/xmtf)  | 11/22 | Bigscience  |
| T0        | [3B](https://huggingface.co/bigscience/T0_3B)/[11B](https://huggingface.co/bigscience/T0pp)                                                                                                                | 0.5K | Enc-Dec      | T5         | [P3](https://huggingface.co/datasets/bigscience/P3) (Multitask)                           | [paper](https://arxiv.org/abs/2110.08207), [code](https://github.com/bigscience-workshop/t-zero) | 10/21 | Bigscience  |

## Instruction Finetuned Model

### English

| Model                   | Available Size                                                                                                                                                       | CTX   | Architecture | Base Model | Tuning Data                                                                                                                    | Link                                                                                                                                                     | Date  | Affiliation   |
| ----------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- | ------------ | ---------- | ------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- | ------------- |
| WizardMath `math`     | [7B](https://huggingface.co/WizardLM/WizardMath-7B-V1.0)/[13B](https://huggingface.co/WizardLM/WizardMath-13B-V1.0)/[70B](https://huggingface.co/WizardLM/WizardMath-7B-V1.0) | 4K    | Dec          | LLaMA2     | self-construct (math)                                                                                                          | [paper](https://arxiv.org/abs/2308.09583),[code](https://github.com/nlpxucan/WizardLM/tree/main/WizardMath)                                                    | 08/23 | MSRA          |
| OpenChat                | [13B-2K](https://huggingface.co/openchat/openchat)/[13B-8K](https://huggingface.co/openchat/openchat_8192)                                                                 | 2K/8K | Dec          | LLaMA      | [filterd](https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset) (6K from ShareGPT)                                 | [code](https://github.com/imoneoi/openchat)                                                                                                                 | 07/23 | personal      |
| LongChat                | [7B](https://huggingface.co/lmsys/longchat-7b-16k)/[13B](https://huggingface.co/lmsys/longchat-13b-16k)                                                                    | 16K   | Dec          | LLaMA      | [self-construct](https://lmsys.org/blog/2023-06-29-longchat/#step-2-finetuning-on-curated-conversation-data) (FastChat pipeline) | [code](https://github.com/DachengLi1/LongChat), [blog](https://lmsys.org/blog/2023-06-29-longchat)                                                             | 06/23 | LMSYS         |
| Tulu                    | [65B](https://huggingface.co/allenai/tulu-65b)                                                                                                                          | 2K    | Dec          | LLaMA      | Tulu mix (Human+GPT mixture)                                                                                                   | [paper](https://arxiv.org/abs/2306.04751), [code](https://github.com/allenai/open-instruct)                                                                   | 06/23 | AI2           |
| WizardCoder `code`    | [15B](https://huggingface.co/WizardLM/WizardCoder-15B-V1.0)                                                                                                             | 8K    | Dec          | StarCoder  | (code)                                                                                                                         | [paper](https://arxiv.org/abs/2306.08568), [code](https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder)                                                  | 06/23 | MSRA          |
| WizardLM                | [7B](https://huggingface.co/WizardLM/WizardLM-7B-V1.0)/[13B](https://huggingface.co/WizardLM/WizardLM-13B-V1.0)/[30B](https://huggingface.co/WizardLM/WizardLM-30B-V1.0)      | 2K    | Dec          | LLaMA      | [Evol-Instruct](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k) (143K instruction)                      | [paper](https://arxiv.org/abs/2304.12244), [code](https://github.com/nlpxucan/WizardLM)                                                                        | 06/23 | MSRA          |
| Lion                    | [7B](https://huggingface.co/YuxinJiang/Lion)                                                                                                                            | 2K    | Dec          | LLaMA      | distill (70K instruction)                                                                                                      | [paper](https://arxiv.org/abs/2305.12870), [code](https://github.com/YJiangcm/Lion)                                                                            | 05/23 | HKUST         |
| UltraChat               | [13B](https://huggingface.co/openbmb/UltraLM-13b)                                                                                                                       | 2K    | Dec          | LLamA      | [ultrachat](https://huggingface.co/datasets/stingning/ultrachat) (1.4M dialogues)                                                | [paper](https://arxiv.org/abs/2305.14233), [code](https://github.com/thunlp/UltraChat)                                                                         | 05/23 | THU           |
| Dromedary               | [65B](https://huggingface.co/zhiqings/dromedary-65b-lora-delta-v0)                                                                                                      | 2K    | Dec          | LLaMA      | [self-aligned](https://huggingface.co/datasets/zhiqings/dromedary-65b-verbose-clone-v0) (360K instruction)                       | [paper](https://arxiv.org/abs/2305.03047), [code](https://github.com/IBM/Dromedary)                                                                           | 05/23 | IBM           |
| Dolly-v2                | [12B](https://huggingface.co/databricks/dolly-v2-12b)                                                                                                                   | 2K    | Dec          | Pythia     | [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) (15k instruction/response)               | [code](https://github.com/databrickslabs/dolly), [blog](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) | 04/23 | Databricks    |
| Baize-v2 (白泽)         | [7B](https://huggingface.co/project-baize/baize-v2-7b)/[13B](https://huggingface.co/project-baize/baize-v2-13b)                                                            | 2K    | Dec          | LLaMA      | self-consturct (100K dialogues from chatgpt)                                                                                   | [paper](https://arxiv.org/abs/2304.01196), [code](https://github.com/project-baize/baize)                                                                     | 04/23 | UCSD          |
| Koala                   | [7B](https://huggingface.co/young-geng/koala/tree/main)/[13B](https://huggingface.co/young-geng/koala/tree/main)                                                           | 2K    | Dec          | LLaMA      | mixture (instruction),*[Koala-test-set](https://github.com/arnav-gudibande/koala-test-set)*                                     | [blog](https://bair.berkeley.edu/blog/2023/04/03/koala/), [code](https://github.com/young-geng/EasyLM/blob/main/docs/koala.md)                                 | 04/23 | UCB           |
| Robin                   | 7B/13B/33B/65B,[Original](https://github.com/OptimalScale/LMFlow#model-zoo)                                                                                             | 2K    | Dec          | LLaMA      | [LMFlow-data](http://lmflow.org:5000/lmflow_data.tar.gz)                                                                          | [paper](https://arxiv.org/abs/2306.12420), [code](https://github.com/OptimalScale/LMFlow)                                                                      | 04/23 | OptimalScale  |
| GPT-NeoXT-Chat-Base-20B | [20B](https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B)                                                                                                  | 2K    | Dec          | GPT-NeoX   | [OIG](https://laion.ai/blog/oig-dataset/) (43M instructions)                                                                     | [code](https://github.com/togethercomputer/OpenChatKit)                                                                                                     | 03/23 | Together, etc |
| Alpaca                  | 7B                                                                                                                                                                   | 2K    | Dec          | LLaMA      | [alpaca](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json) (52K instructions)                             | [code](https://github.com/tatsu-lab/stanford_alpaca), [blog](https://crfm.stanford.edu/2023/03/13/alpaca.html)                                                | 03/23 | Stanford      |
| Vicuna                  | [7B](https://huggingface.co/lmsys/vicuna-7b-v1.3)/[13B](https://huggingface.co/lmsys/vicuna-13b-v1.3)/[33B](https://huggingface.co/lmsys/vicuna-33b-v1.3)                     | 2K    | Dec          | LLaMA      | [ShareGPT](https://yougithub.com/lm-sys/FastChat#data) (70K conversations)                                                       | [code](yougithub.com/lm-sys/FastChat), [blog](https://lmsys.org/blog/2023-03-30-vicuna/)                                                                       | 03/23 | LMSYS         |

*Toys*: [OpenAlpaca](https://github.com/yxuansu/OpenAlpaca) (openllama-7b+INST), [Parakeets](https://github.com/OptimalScale/LMFlow#model-zoo) (GPT-NEO-2.7B + INST), [Cockatoo](https://github.com/OptimalScale/LMFlow#model-zoo) (StableLM-3B/7B + INST), [Camel](https://huggingface.co/Writer/camel-5b-hf) (5B/30B, [palmyra](https://huggingface.co/Writer/palmyra-base) + INST), [GALPACA](https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b) (GALACTICA-30B + INST), [Flan-Alpaca](https://github.com/declare-lab/flan-alpaca) (Flan-T5 + INST),

### Chinese

| Model                | Available Size                                                                                                                                                                                                                                                                                                  | CTX | Architecture | Base Model      | Tuning Data                                                                                                                                                                                                                                          | Link                                                                                                | Date  | Affiliation |
| -------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --- | ------------ | --------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- | ----- | ----------- |
| Firefly (流萤)       | [2.6B](https://huggingface.co/YeungNLP/firefly-bloom-2b6-sft-v2)/[7.1B](https://huggingface.co/YeungNLP/firefly-baichuan-7b-qlora-sft)                                                                                                                                                                                | 2K  | Dec          | BLOOM/baichuan  | [firefly-train-1.1M](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M), [moss-003-sft-data](https://huggingface.co/datasets/YeungNLP/moss-003-sft-data), [ultrachat](https://huggingface.co/datasets/YeungNLP/ultrachat) (3M+ Mixture)           | [code](https://github.com/yangjianxin1/Firefly)                                                        | 05/23 | personal    |
| BELLE                | [7B](https://huggingface.co/BelleGroup/BELLE-7B-2M)/[LLaMA-7B](https://huggingface.co/BelleGroup/BELLE-LLaMA-EXT-7B)/[LLaMA-13B](https://huggingface.co/BelleGroup/BELLE-LLaMA-EXT-13B)                                                                                                                                  | 2K  | Dec          | BLOOMZ-mt/LLaMA | [BELLE](https://github.com/LianjiaTech/BELLE/tree/main/data) (1-10M instruction)                                                                                                                                                                       | [paper](https://github.com/LianjiaTech/BELLE/tree/main/docs),[code](https://github.com/LianjiaTech/BELLE) | 05/23 | Lianjia     |
| Linly-ChatFlow       | [7B](https://huggingface.co/Linly-AI/ChatFlow-7B)/[13B](https://huggingface.co/Linly-AI/ChatFlow-7B)                                                                                                                                                                                                                  | 2K  | Dec          | LLaMA           | self-construct (Continual Pretrain + Chinese Dialogue)                                                                                                                                                                                               | [code](https://github.com/CVI-SZU/Linly)                                                               | 04/23 | SZU         |
| 骆驼 (Luotuo)        | 7B                                                                                                                                                                                                                                                                                                              | 2K  | Dec          | LLaMA           | [丝绸之路](https://github.com/LC1332/Luotuo-Silk-Road#%E4%B8%9D%E7%BB%B8%E4%B9%8B%E8%B7%AF-%E6%9E%84%E5%BB%BA%E4%B8%AD%E6%96%87%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80) (52K Translated Alpaca, QA) | [code](https://github.com/LC1332/Luotuo-Chinese-LLM)                                                   | 04/23 | SenseTime   |
| Chinese-Alpaca-LLaMA | [7B](https://huggingface.co/ziqingyang/chinese-alpaca-plus-lora-7b)/[13B](https://huggingface.co/ziqingyang/chinese-alpaca-plus-lora-13b)/[33B](https://huggingface.co/ziqingyang/chinese-alpaca-lora-33b),[Original](https://github.com/ymcui/Chinese-LLaMA-Alpaca#%E6%8E%A8%E8%8D%90%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B) | 2K  | Dec          | LLaMA           | additional pretrain,[self-construct-tune](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE) (2M+ Mixture)                                                                  | [paper](https://arxiv.org/abs/2304.08177) ,[code](https://github.com/ymcui/Chinese-LLaMA-Alpaca)          | 04/23 | CLUE        |
| Chinese-Vicuna       | [7B](https://huggingface.co/Facico)                                                                                                                                                                                                                                                                                | 2K  | Dec          | LLaMA           | [Belle](https://github.com/LianjiaTech/BELLE), [guanaco](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset) (1M instructions)                                                                                                                 | [code](https://github.com/Facico/Chinese-Vicuna)                                                       | 04/23 | personal    |

*Toys*: [InstructGLM](https://github.com/yanqiangmiffy/InstructGLM) (ChatGLM-6b+INST)

### Multilingual

| Model                     | Available Size                                                                                                                                                                        | CTX                   | Architecture          | Base Model                                           | Tuning Data                                                                                                                                                                                                                      | Link                                                                                                                                                                                                                                                                                                                             | Date  | Affiliation     |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------- | --------------------- | ---------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- | --------------- |
| Intern-LM-Chat            | [7B](https://huggingface.co/internlm/internlm-chat-7b-8k)                                                                                                                                | 8K                    | Dec                   | InternLM                                             | [alpaca](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)                                                                                                                                                   | [code](https://github.com/InternLM/InternLM)                                                                                                                                                                                                                                                                                        | 07/23 | SenseTime       |
| Tigerbot                  | [7B](https://huggingface.co/TigerResearch/tigerbot-7b-sft)/[180B](https://huggingface.co/TigerResearch/tigerbot-180b-research)                                                              | 2K                    | Dec                   | Tigerbot-base                                        | [self-construct](https://github.com/TigerResearch/TigerBot#%E5%BE%AE%E8%B0%83%E6%95%B0%E6%8D%AE) (1M instruction, English&Chinese)                                                                                                  | [code](https://github.com/TigerResearch/TigerBot)                                                                                                                                                                                                                                                                                   | 06/23 | Tigerobo        |
| BayLing (百聆)            | [7B](https://huggingface.co/ICTNLP/bayling-7b-diff)/[13B](https://huggingface.co/ICTNLP/bayling-13b-diff)                                                                                   | 2K                    | Dec                   | LLaMA                                                | self-construct (?, English&Chinese),*[BayLing-80 Test Set](https://github.com/ictnlp/BayLing/blob/main/data/BayLing-80)*                                                                                                          | [paper](https://arxiv.org/abs/2306.10968), [code](https://github.com/ictnlp/BayLing)                                                                                                                                                                                                                                                   | 06/23 | CAS             |
| ZhiXi                     | [13B](https://huggingface.co/zjunlp/zhixi-13b-diff), [Original](https://github.com/zjunlp/KnowLM#2-2)                                                                                       | 2K                    | Dec                   | LLaMA                                                | self-construct([additional pretraining](https://github.com/zjunlp/KnowLM#31-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA),  mixture+[KG2Instruction](https://arxiv.org/abs/2305.11527), English&Chinese) | [code](https://github.com/zjunlp/KnowLM)                                                                                                                                                                                                                                                                                            | 05/23 | ZJU             |
| LLaMA-Adapter/V2          | [7B](https://github.com/OpenGVLab/LLaMA-Adapter/releases/download/v.1.0.0/llama_adapter_len10_layer30_release.pth)/[7B-v2](https://github.com/OpenGVLab/LLaMA-Adapter/releases/tag/v.2.0.0) | 2K                    | Dec+**Adapter** | LLaMA                                                | [alpaca](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json) (52K instructions, English & Chinese), visual instruction (v2)                                                                                   | [paper](https://arxiv.org/abs/2303.16199)/[v2](https://arxiv.org/abs/2304.15010), [code](https://github.com/OpenGVLab/LLaMA-Adapter)                                                                                                                                                                                                     | 05/23 | Shanghai AI Lab |
| ChatGLM/V2 `Commercial` | [6B](https://huggingface.co/THUDM/chatglm-6b)/[V2](https://huggingface.co/THUDM/chatglm2-6b)                                                                                                | 2K/8K_Train-32K_Infer | Enc-Dec               | -                                                    | self-construct (1T/1.4T tokens Pretraining data, ? Aligned data, English&Chinese)                                                                                                                                                | [code](https://github.com/THUDM/ChatGLM-6B)/[v2](https://github.com/THUDM/ChatGLM2-6B), [blog](https://chatglm.cn/blog)                                                                                                                                                                                                                  | 05/23 | THU             |
| Phoenix                   | [7B-inst](https://huggingface.co/FreedomIntelligence/phoenix-chat-7b)/[7B-inst-chat](https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b)                                       | 2K                    | Dec                   | BLOOMZ-mt                                            | [phoenix-sft-data](https://huggingface.co/datasets/FreedomIntelligence/phoenix-sft-data-v1) (267K instruction, 189K conversation, non-latin)                                                                                      | [paper](https://arxiv.org/abs/2304.10453),[code](https://github.com/FreedomIntelligence/LLMZoo)                                                                                                                                                                                                                                        | 04/23 | CUHKSZ          |
| Chimera                   | [7B](https://huggingface.co/FreedomIntelligence/chimera-inst-chat-7b-delta)/[13B](https://huggingface.co/FreedomIntelligence/chimera-inst-chat-7b-delta)                                    | 2K                    | Dec                   | LLaMA                                                | [phoenix-sft-data](https://huggingface.co/datasets/FreedomIntelligence/phoenix-sft-data-v1) (267K instruction, 189K conversation, latin)                                                                                          | [paper](https://arxiv.org/abs/2304.10453),[code](https://github.com/FreedomIntelligence/LLMZoo)                                                                                                                                                                                                                                        | 04/23 | CUHKSZ          |
| Coati                     | [7B]()                                                                                                                                                                                   | 2K                    | Dec                   | LLaMA                                                | [InstructionWild](https://github.com/XueFuzhao/InstructionWild) (104K instruction, English&Chinese), **RLHF**                                                                                                                | [code](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat), [blog](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b), [demo](https://www.youtube.com/watch?v=HcTiHzApHm0), [tutorial](https://www.youtube.com/watch?v=-qFBZFmOJfg) | 04/23 | CollossalAI     |
| ChatRWKV (Raven/World)    | 7B/14B,[Raven](https://huggingface.co/BlinkDL/rwkv-4-raven), [World](https://huggingface.co/BlinkDL/rwkv-4-world)                                                                           | -                     | RWKV                  | RWKV-4                                               | self-construct (mixture for Raven, multilingual for World)                                                                                                                                                                       | [code](https://github.com/BlinkDL/ChatRWKV)                                                                                                                                                                                                                                                                                         | 04/23 | BlinkDL         |
| MOSS-003                  | [16B](https://huggingface.co/fnlp/moss-moon-003-sft)                                                                                                                                     | 2K                    | Dec                   | [moss-moon-003-base](https://github.com/OpenLMLab/MOSS) | [Moss-003-sft-data](https://github.com/OpenLMLab/MOSS/tree/main/SFT_data) (1M Conversations, English&Chinese)                                                                                                                      | [code](https://github.com/OpenLMLab/MOSS)                                                                                                                                                                                                                                                                                           | 04/23 | FDU             |
| Guanaco                   | [7B](https://huggingface.co/JosephusCheung/Guanaco)                                                                                                                                      | 2K                    | Dec                   | LLaMA                                                | [Guanaco](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset) (534K, Multilingual)                                                                                                                                      | [blog](https://guanaco-model.github.io/)                                                                                                                                                                                                                                                                                            | 04/23 | personal        |
| Alpaca-CoT                | 7B                                                                                                                                                                                    | 2K                    | Dec                   | LLaMA                                                | [collection](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT) (mainly CoT, Multilingual)                                                                                                                                       | [code](https://github.com/PhoebusSi/Alpaca-CoT)                                                                                                                                                                                                                                                                                     | 04/23 | personal        |

*Toys*: [Cabrita](https://github.com/22-hours/cabrita) (LLaMA-7B + Alpaca-portuguese), [BilLLa](https://github.com/Neutralzz/BiLLa) (LLaMA-7B + continual pretrain + INST, English&Chinese)

## Human Feedback Finetuned Models

| Model           | Available Size                                                                                                                                                      | CTX                   | Architecture | Base Model | Tuning Data                                                                                                               | Link                                                                                                                                                                                                                                                                                                                             | Date  | Affiliation  |
| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------- | ------------ | ---------- | ------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- | ------------ |
| Llama 2-Chat-hf | [7B](https://huggingface.co/meta-llama/Llama-2-7b-chat),[13B](https://huggingface.co/meta-llama/Llama-2-13b-chat), [70B](https://huggingface.co/meta-llama/Llama-2-70b-chat) | 4K                    | Dec          | Llama 2    | self-construct + RLHF                                                                                                     | [code](https://github.com/facebookresearch/llama), [blog](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)                                                                                                                                                                               | 07/23 | Meta         |
| Ziya-v1         | [7B](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-7B-Reward)                                                                                                            | 2K                    | Dec          | LLaMA      | self-construct (English&Chinese) + RLHF                                                                                   | [code](https://github.com/IDEA-CCNL/Fengshenbang-LM#%E5%A7%9C%E5%AD%90%E7%89%99%E7%B3%BB%E5%88%97)                                                                                                                                                                                                                                  | 06/23 | IDEA         |
| ChatGLM/V2      | [6B](https://huggingface.co/THUDM/chatglm-6b)/[6B-V2](https://huggingface.co/THUDM/chatglm2-6b)                                                                           | 2K/8K_Train-32K_Infer | Enc-Dec      | -          | self-construct (? Aligned data, English&Chinese) + RLHF                                                                   | [code](https://github.com/THUDM/ChatGLM-6B)/[v2](https://github.com/THUDM/ChatGLM2-6B), [blog](https://chatglm.cn/blog)                                                                                                                                                                                                                  | 05/23 | THU          |
| Wombat          | [7B](https://huggingface.co/GanjinZero/wombat-7b-delta)/[7B-GPT4](https://huggingface.co/GanjinZero/wombat-7b-gpt4-delta)                                                 | 2K                    | Dec          | Alpaca     | [self-construct](https://github.com/GanjinZero/RRHF#data-and-training) + RRHF                                               | [paper](https://arxiv.org/abs/2304.05302), [code](https://github.com/GanjinZero/RRHF)                                                                                                                                                                                                                                                 | 04/23 | DAMO         |
| StableVicuna    | [13B](https://huggingface.co/CarperAI/stable-vicuna-13b-delta/)                                                                                                        | 2K                    | Dec          | LLaMA      | Vicuna-13B + RLHF                                                                                                         | [code](https://github.com/Stability-AI/StableLM#stablevicuna)                                                                                                                                                                                                                                                                       | 04/23 | Stability.AI |
| StackLLaMA      | [7B](https://huggingface.co/trl-lib/llama-7b-se-rl-peft)                                                                                                               | 2K                    | Dec          | LLaMA      | [StackExchange dataset](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences) (10M instructions) + RLHF | [blog](https://huggingface.co/blog/stackllama)                                                                                                                                                                                                                                                                                      | 04/23 | Huggingface  |
| Coati           | [7B]()                                                                                                                                                                 | 2K                    | Dec          | LLaMA      | [InstructionWild](https://github.com/XueFuzhao/InstructionWild) (104K instruction, English&Chinese) + RLHF                  | [code](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat), [blog](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b), [demo](https://www.youtube.com/watch?v=HcTiHzApHm0), [tutorial](https://www.youtube.com/watch?v=-qFBZFmOJfg) | 04/23 | CollossalAI  |

## Domain Finetuned Model

| Model                       | Available Size                                                                                                                                 | CTX | Base Model | Domain  | Tuning Data                                                                                              | Link                                                                                  | Date  | Affiliation |
| --------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- | --- | ---------- | ------- | -------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- | ----- | ----------- |
| ChatLaw                     | 13B/33B                                                                                                                                        | 2K  | LLaMA      | Law     | [self-construct](https://github.com/PKU-YuanGroup/ChatLaw#%E6%95%B0%E6%8D%AE-dataset) (Chinese)            | [paper](https://arxiv.org/abs/2306.16092), [code](https://github.com/PKU-YuanGroup/ChatLaw) | 07/23 | PKU         |
| Lawyer-LLamA                | [13B](https://huggingface.co/pkupie/lawyer-llama-13b-beta1.0), [Original](https://github.com/AndrewZhe/lawyer-llama/blob/main/demo/run_inference.md) | 2K  | LLaMA      | Law     | [self-construct](https://github.com/AndrewZhe/lawyer-llama#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE) (Chinese) | [code](https://github.com/AndrewZhe/lawyer-llama)                                        | 05/23 | PKU         |
| medAlpaca                   | [7B](https://huggingface.co/medalpaca/medalpaca-7b)/[13B](https://huggingface.co/medalpaca/medalpaca-13b)                                            | 2K  | LLaMA      | Medical | [self-construct](https://github.com/kbressem/medAlpaca#data)                                                | [paper](https://arxiv.org/abs/2304.08247), [code](https://github.com/kbressem/medAlpaca)   | 04/23 | TUM         |
| BenTsao/HuaTuo (本草/华佗) | 7B,[Original](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese#%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD)                                           | 2K  | LLaMA      | Medical | self-construct ([cMeKG](https://github.com/king-yyf/CMeKG_tools), Chinese)                                  | [code](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese)                              | 03/23 | HIT         |
| ChatDoctor                  | 7B,[Original](https://drive.google.com/drive/folders/11-qPzz9ZdHD6pc47wBSOUSU61MaDPyRh?usp=sharing)                                               | 2K  | LLaMA      | Medical | [ChatDoctor Dataset](https://github.com/Kent0n-Li/ChatDoctor#data-and-model)                                | [code](https://github.com/Kent0n-Li/ChatDoctor)                                          | 03/23 | personal    |

*Toys*: [Robin-medical](https://github.com/OptimalScale/LMFlow#model-zoo) (LLamA+INST-medical)

## Open Source Projects

Additional open source projects for LLM research

### reproduction/framework

#### (1) Tuning

- [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama): independent implementation of LLaMA pretraining, finetuning, and inference code that is fully open source under the Apache 2.0 license
  - *by Lightning, 2023.05*
- [Llama-X](https://github.com/AetherCortex/Llama-X): Open Academic Research on Improving LLaMA to SOTA LLM
  - *by MSRA, 2023.05*
- [LLMZoo](https://github.com/FreedomIntelligence/LLMZoo): A project that provides data, models, and evaluation benchmark for large language models
  - *by CUHKAZ, 2023.05*
  - chat-model: [Phonex](https://github.com/FreedomIntelligence/LLMZoo) (BLOOMZ-7b-mt + INST), [Chimera](https://github.com/FreedomIntelligence/LLMZoo) (LLaMA-7b/13b + INST)
- [OpenChatKit](https://github.com/togethercomputer/OpenChatKit): OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications
  - chat-model: [Pythia-Chat-Base-7B](https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B), [GPT-NeoXT-Chat-Base-20B](https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B)
- [Alpaca-CoT](https://github.com/PhoebusSi/Alpaca-CoT): We unified the interfaces of instruction-tuning data (e.g., CoT data), multiple LLMs and parameter-efficient methods (e.g., lora, p-tuning) together for easy use. Meanwhile, we created a new branch to build a Tabular LLM.（我们分别统一了丰富的IFT数据（如CoT数据，目前仍不断扩充）、多种训练效率方法（如lora，p-tuning）以及多种LLMs，三个层面上的接口，打造方便研究人员上手的LLM-IFT研究平台。同时tabular_llm分支构建了面向表格智能任务的LLM。
  - *2023.04*
- [Linly](https://github.com/CVI-SZU/Linly): 本项目向社区提供 **中文对话模型 Linly-ChatFlow 、中文基础模型 Chinese-LLaMA、Chinese-Falcon 及其训练数据** 。 模型基于 [TencentPretrain](https://github.com/Tencent/TencentPretrain) 预训练框架全参数训练（Full-tuning），此外，本项目还公开从头训练的 [**Linly-OpenLLaMA**](https://github.com/CVI-SZU/Linly/wiki/Linly-OpenLLaMA) 模型，包含 **3B、7B、13B** 规模，在 1TB 中英文语料预训练，针对中文优化字词结合tokenizer，模型以 Apache 2.0 协议公开。
  - chat-model: [Linly-Chinese-Falcon](https://github.com/CVI-SZU/Linly#linly-chinese-falcon), [Linly-Chinese-LLaMA](https://github.com/CVI-SZU/Linly#linly-chinese-llama)
  - *by SZU, 2023.04*
- [LMFlow](https://github.com/OptimalScale/LMFlow): An extensible, convenient, and efficient toolbox for finetuning large machine learning models
  - *by OptimalScale, 2023.03,* [paper](https://arxiv.org/abs/2306.12420)
  - chat-model: [Robin](https://github.com/OptimalScale/LMFlow#model-zoo) (LLaMA+INST), [Parakeets](https://github.com/OptimalScale/LMFlow#model-zoo) (GPT-NEO-2.7B + INST), [Cockatoo](https://github.com/OptimalScale/LMFlow#model-zoo) (StableLM-3B/7B + INST)

#### (2) RLHF

- [PKU Beaver](https://github.com/PKU-Alignment/safe-rlhf): Beaver is a highly modular open-source RLHF framework developed by the PKU-Alignment team at Peking University
  - *by PKU, 2023.05*, [PKU-RLHF-Dataset](https://github.com/PKU-Alignment/safe-rlhf#pku-saferlhf-dataset)
- [PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch): Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture
  - *2023.04*
- [ChatLLaMA](https://github.com/nebuly-ai/nebuly/tree/main/optimization/chatllama): ChatLLaMA has been designed to help developers with various use cases, all related to RLHF training and optimized inference
  - *by Nebuly.AI, 2023.04*
- [DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat): A fast, affordable, scalable and open system framework for enabling end-to-end Reinforcement Learning Human Feedback (RLHF) training experience to generate high-quality ChatGPT-style models at all scales
  - *by DeepSpeed, 2023.04*
- [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat): a project to implement LLM with RLHF, powered by the Colossal-AI project
  - *by Collosal.AI, 2023.03*
  - chat-model: Coati-7B

#### (3) Specific domain

- [FinGPT](https://github.com/AI4Finance-Foundation/FinGPT): Open-source for open finance
  - domain: finance

### accelerate

- [QLoRA](https://github.com/artidoro/qlora): an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance
  - *by UW NLP, 2023.05*, [paper](https://arxiv.org/abs/2305.14314)
- [LLaMA.cpp](https://github.com/ggerganov/llama.cpp): Inference of LLaMA model in pure C/C++
  - *2023.04*
- [GPTQ-for-LLaMA](https://github.com/qwopqwop200/GPTQ-for-LLaMa): 4 bits quantization of LLaMA using GPTQ
  - *2023.04*
- [Alpaca-LoRA](https://github.com/tloen/alpaca-lora): This repository contains code for reproducing the Stanford Alpaca results using low-rank adaptation (LoRA)
  - *2023.03*

### evaluation

- [PandaLM](https://github.com/WeOpenML/PandaLM): PandaLM aims to provide reproducible and automated comparisons between different large language models (LLMs).
  - *by Westlake Univ. 2023.05 [paper](https://arxiv.org/abs/2306.05087)*

### deployment/demo

- [vLLM](https://github.com/vllm-project/vllm): A high-throughput and memory-efficient inference and serving engine for LLMs
- [OpenAssistant](https://github.com/LAION-AI/Open-Assistant): Open Assistant is a project meant to give everyone access to a great chat based large language model.
  - *by LAION-AI, 2023.04*, [doc](https://projects.laion.ai/Open-Assistant/docs/intro), [demo](https://huggingface.co/chat)
- [GPT4ALL](https://github.com/nomic-ai/gpt4all): GPT4All is an ecosystem to train and deploy **powerful** and **customized** large language models that run locally on consumer grade CPUs
  - *2023.03*
- [h2oGPT](https://github.com/h2oai/h2ogpt): h2oGPT is a large language model (LLM) fine-tuning framework and chatbot UI with document(s) question-answer capabilities.
  - *by H2O.ai, 2023.03*
- [LangChain](https://github.com/hwchase17/langchain): Building applications with LLMs through composability
  - tutorial: [LanChain中文入门教程](https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide)
  - *2023.03*

## Reference

- [open-llms](https://github.com/eugeneyan/open-llms), A list of open LLMs available for commercial use.
- [Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM), 整理开源的中文大语言模型，以规模较小、可私有化部署、训练成本较低的模型为主，包括底座模型，垂直领域微调及应用，数据集与教程等
- [LLM-Zoo](https://github.com/DAMO-NLP-SG/LLM-Zoo), collects information of various open- and closed-source LLMs
- [FindTheChatGPTer](https://github.com/chenking2020/FindTheChatGPTer), 汇总那些ChatGPT的开源平替们，包括文本大模型、多模态大模型等
- [中国大模型](https://github.com/wgwang/LLMs-In-China), 旨在记录中国大模型情况
- [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM#open-llm): a curated list of Large Language Model
