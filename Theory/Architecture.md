# Architecture

elegant architecture for **longer context** and **faster speed**

## Papers

| Title                                                                                         | Pub       | Preprint                                    | Supplementary                              |
| --------------------------------------------------------------------------------------------- | --------- | ------------------------------------------- | ------------------------------------------ |
| RWKV: Reinventing RNNs for the Transformer Era                                                |           | [2305.13048](https://arxiv.org/abs/2305.13048) | linear, Blink                              |
| Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder |           | [2304.04052](https://arxiv.org/abs/2304.04052) | encoder-decoder or decoder-only, Cambridge |
| Hyena Hierarchy: Towards Larger Convolutional Language Models                                 | ICML 2023 | [2302.10866](https://arxiv.org/abs/2302.10866) | CNN,Stanford                               |


## Projects
- [Long-Context](https://github.com/abacusai/Long-Context)
    > This repository contains code and tooling for the Abacus.AI LLM Context Expansion project. Also included are evaluation scripts and benchmark tasks that evaluate a modelâ€™s information retrieval capabilities with context expansion. We also include key experimental results and instructions for reproducing and building on them

- [LongBench](https://github.com/THUDM/LongBench)
    > LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding