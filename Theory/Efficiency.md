# Efficiency

## Paper

| Title                                              | Pub       | Preprint                                    | Supplementary   |
| -------------------------------------------------- | --------- | ------------------------------------------- | --------------- |
| LoRA: Low-Rank Adaptation of Large Language Models | ICLR 2022 | [2106.09685](https://arxiv.org/abs/2106.09685) | LoRA, Microsoft |

## Reference

[Awesome-LLM-Compression](https://github.com/HuangOwen/Awesome-LLM-Compression), Awesome LLM compression research papers and tools to accelerate the LLM training and inference
