# Efficiency

## Paper

| Title                                                                          | Pub       | Preprint                                    | Supplementary                                                         |
| ------------------------------------------------------------------------------ | --------- | ------------------------------------------- | --------------------------------------------------------------------- |
| Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning |           | [2310.06694](https://arxiv.org/abs/2310.06694) | [LLM-Shearing](https://github.com/princeton-nlp/LLM-Shearing), Princeton |
| LLM-QAT: Data-Free Quantization Aware Training for Large Language Models       |           | [2305.17888](https://arxiv.org/abs/2305.17888) | Meta                                                                  |
| LoRA: Low-Rank Adaptation of Large Language Models                             | ICLR 2022 | [2106.09685](https://arxiv.org/abs/2106.09685) | LoRA, Microsoft                                                       |
| MoEfication: Transformer Feed-forward Layers are Mixtures of Experts           | ACL 2022  | [2110.01786](https://arxiv.org/abs/2110.01786) | [MoEfication](https://github.com/thunlp/MoEfication), THU                |

## Reference

[Awesome-LLM-Compression](https://github.com/HuangOwen/Awesome-LLM-Compression), Awesome LLM compression research papers and tools to accelerate the LLM training and inference

[Awesome-Long-Context](https://github.com/showlab/Awesome-Long-Context), A curated list of resources about long-context in large-language models and video understanding.
