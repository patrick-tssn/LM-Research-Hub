# Mixture of Experts (MoE)

Table of Content

- [Papers](#papers)
- [Reference](#reference)

## Papers

| Title                                                       | Pub       | Preprint                                    | Supplementary                                                         |
| ----------------------------------------------------------- | --------- | ------------------------------------------- | --------------------------------------------------------------------- |
| Towards Understanding Mixture of Experts in Deep Learning   |           | [2208.02813](https://arxiv.org/abs/2208.02813) | UCLA                                                                  |
| Sparse Mixture-of-Experts are Domain Generalizable Learners | ICLR 2023 | [2206.04046](https://arxiv.org/abs/2206.04046) | [GMoE](https://github.com/Luodian/Generalizable-Mixture-of-Experts), NTU |

## Reference

[last update: 2022.10] [Awesome-Mixture-of-Experts-Papers](https://github.com/codecaution/Awesome-Mixture-of-Experts-Papers), A curated reading list of research in Mixture-of-Experts(MoE).
