# Mixture of Experts (MoE)

Table of Content

- [Papers](#papers)
- [Blogs](#blogs)
- [Reference](#reference)

## Papers

| Title                                                                                       | Pub           | Preprint                                    | Supplementary                                                                                             |
| ------------------------------------------------------------------------------------------- | ------------- | ------------------------------------------- | --------------------------------------------------------------------------------------------------------- |
| COMET: Learning Cardinality Constrained Mixture of Experts with Trees and Local Search      | KDD 2023      | [2306.02824](https://arxiv.org/abs/2306.02824) | MIT                                                                                                       |
| Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models |               | [2305.14705](https://arxiv.org/abs/2305.14705) | Google                                                                                                    |
| PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing     |               | [2303.10845](https://arxiv.org/abs/2303.10845) | Huawei                                                                                                    |
| Towards Understanding Mixture of Experts in Deep Learning                                   |               | [2208.02813](https://arxiv.org/abs/2208.02813) | UCLA                                                                                                      |
| Sparse Mixture-of-Experts are Domain Generalizable Learners                                 | ICLR 2023     | [2206.04046](https://arxiv.org/abs/2206.04046) | [code (GMoE)](https://github.com/Luodian/Generalizable-Mixture-of-Experts), NTU                              |
| Mixture-of-Experts with Expert Choice Routing                                               | NeurIPS 2022  | [2202.09368](https://arxiv.org/abs/2202.09368) | [blog](https://ai.googleblog.com/2022/11/mixture-of-experts-with-expert-choice.html), Google                 |
| Taming Sparsely Activated Transformer with Stochastic Experts                               | ICLR 2022     | [2110.04260](https://arxiv.org/abs/2110.04260) | [code (THOR)](https://github.com/microsoft/Stochastic-Mixture-of-Experts), random routing,Microsoft          |
| Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference                  | EMNLP 2021    | [2110.03742](https://arxiv.org/abs/2110.03742) | Google                                                                                                    |
| Hash Layers For Large Sparse Models                                                         | NeurIPS 2021 | [2106.04426](https://arxiv.org/abs/2106.04426) | [code](https://github.com/facebookresearch/ParlAI/tree/main/projects/params_vs_compute), hash routing, Meta |

## Blogs

- [2022-11] [Mixture-of-Experts with Expert Choice Routing](https://ai.googleblog.com/2022/11/mixture-of-experts-with-expert-choice.html), Google

## Reference

[last update: 2022.10] [Awesome-Mixture-of-Experts-Papers](https://github.com/codecaution/Awesome-Mixture-of-Experts-Papers), A curated reading list of research in Mixture-of-Experts(MoE).
