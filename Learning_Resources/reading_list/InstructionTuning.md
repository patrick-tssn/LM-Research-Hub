# Instruction Tuning

Table of Content

- [Papers](#papers)
- [Blogs](#blogs)
- [Reference](#reference)

## Papers

Survey

- [2023-07] Aligning Large Language Models with Human: A Survey, [paper](https://arxiv.org/abs/2307.12966), [github](https://github.com/GaryYufei/AlignLLMHumanSurvey)

Reading List

| Title                                                                                          | Pub          | Date                                          | Supplementary                                                          |
| ---------------------------------------------------------------------------------------------- | ------------ | --------------------------------------------- | ---------------------------------------------------------------------- |
| How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources             |              | [2306.04751](https://arxiv.org/abs/2306.04751)   | [open-instruct](https://github.com/allenai/open-instruct) (code), AI2     |
| LIMA: Less Is More for Alignment                                                               |              | [2305.11206](https://arxiv.org/abs/2305.11206)   | [lima](https://huggingface.co/datasets/GAIR/lima) (data), Meta            |
| Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision |              | [2305.03047](https://arxiv.org/abs/2305.03047)   | [Dromedary](https://github.com/IBM/Dromedary) (code), IBM                 |
| Self-Instruct: Aligning LM with Self Generated Instructions                                    | ACL 2023     | [2212.10560](https://arxiv.org/abs/2212.10560)   | [self-instruct](https://github.com/yizhongw/self-instruct)(code), UW      |
| Scaling Instruction-Finetuned Language Models                                                  |              | [2210.11416](https://arxiv.org/abs/2210.11416)   | FlanV2, Google                                                         |
| Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks      | ACL 2023     | [2204.07705](https://arxiv.org/abs/2204.07705)   | Natural-Instruction V2, AI2                                            |
| Training language models to follow instructions with human feedback                            | NeurIPS 2022 | [2203.02155](https://arxiv.org/abs/2203.02155)   | RLHF, OpenAI                                                           |
| Finetuned Language Models are Zero-Shot Learners                                               | ICLR 2022    | [2109.01652](https://arxiv.org/abs/2109.01652)   | FlanV1, Google                                                         |
| Cross-Task Generalization via Natural Language Crowdsourcing Instructions                      | ACL 2022     | [2104.08773](https://arxiv.org/abs/2104.08773v4) | [natural-instruction](https://instructions.apps.allenai.org/) (data), AI2 |

*date: YYMM.ArxivID*

## Blogs

| Title                                                                                                               | Date    | Supplementary                                                          |
| ------------------------------------------------------------------------------------------------------------------- | ------- | ---------------------------------------------------------------------- |
| [Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality](https://lmsys.org/blog/2023-03-30-vicuna/) | 2023.03 | [Vicuna](https://github.com/lm-sys/FastChat#model-weights), LMSYS         |
| [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)           | 2023.03 | [stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca), Stanford |

## Reference

- [Instruction Tuning阶段性总结](https://yaofu.notion.site/2023-06-Instruction-Tuning-935b48e5f26448e6868320b9327374a1), by 符尧 (Yao Fu), 2023.06
- [Instruction-Tuning-Papers](https://github.com/SinclairCoder/Instruction-Tuning-Papers), A trend starts from Natrural-Instruction (ACL 2022), FLAN (ICLR 2022) and T0 (ICLR 2022).
