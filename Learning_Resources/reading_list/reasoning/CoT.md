# Chain of Thought

## Papers

| Title                                                                   | Pub          | Preprint                                    | Supplementary                                                                |
| ----------------------------------------------------------------------- | ------------ | ------------------------------------------- | ---------------------------------------------------------------------------- |
| Reasoning with language model is planning with world model              |              | [2305.14992](https://arxiv.org/abs/2305.14992) | [llm-reasoners](https://github.com/Ber666/llm-reasoners), UCSD                  |
| Tree of Thoughts: Deliberate Problem Solving with Large Language Models |              | [2305.10601](https://arxiv.org/abs/2305.10601) | [code (Tree of Thought)](https://github.com/kyegomez/tree-of-thoughts),Â Google |
| Decomposed Prompting: A Modular Approach for Solving Complex Tasks      | ICLR 2023    | [2210.02406](https://arxiv.org/abs/2210.02406) | AI2                                                                          |
| Chain-of-Thought Prompting Elicits Reasoning in Large Language Models   | Neurips 2022 | [2201.11903](https://arxiv.org/abs/2201.11903) | Google                                                                       |

## Reference

- [Chain-of-ThoughtsPapers](https://github.com/Timothyxxx/Chain-of-ThoughtsPapers), A trend starts from "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
- [Reasoning in Large Language Models](https://github.com/atfortes/LLM-Reasoning-Papers)
