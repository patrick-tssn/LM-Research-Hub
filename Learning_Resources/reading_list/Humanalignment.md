# Human Alignment

## Table of Contents

- [Papers](#papers)

## Papers

Surveys

- [2023.09] AI Alignment: A Comprehensive Survey, [paper](https://arxiv.org/abs/2310.19852), [blog](https://alignmentsurvey.com/)

| Title                                                                                                           | Pub | Preprint                                                                | Supplementary                                                                               |
| --------------------------------------------------------------------------------------------------------------- | --- | ----------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- |
| How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs |     | [2401.github](https://www.yi-zeng.com/wp-content/uploads/2024/01/view.pdf) | [persuasive_jailbreaker](https://github.com/CHATS-lab/persuasive_jailbreaker),   Virginia Tech |
|         SALMON: Self-Alignment with Principle-Following Reward Models                                                                                                        |     |                                                                  [2310.05910](https://arxiv.org/abs/2310.05910)       |                                                                                      [SALMON](https://github.com/IBM/SALMON),    MiT-IMB   |
