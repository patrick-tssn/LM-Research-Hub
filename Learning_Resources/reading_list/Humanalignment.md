# Human Alignment

## Table of Contents

- [Papers](#papers)

## Papers

Surveys

- [2023.09] AI Alignment: A Comprehensive Survey, [paper](https://arxiv.org/abs/2310.19852), [blog](https://alignmentsurvey.com/)

| Title                                                                                         | Pub          | Preprint                                    | Supplementary                                                                        |
| --------------------------------------------------------------------------------------------- | ------------ | ------------------------------------------- | ------------------------------------------------------------------------------------ |
| How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs                              |              | [2401.github](https://www.yi-zeng.com/wp-content/uploads/2024/01/view.pdf) | [persuasive_jailbreaker](https://github.com/CHATS-lab/persuasive_jailbreaker),   Virginia Tech                       |

## Datasets & Benchmarks

- [TrustLLM: Trustworthiness in Large Language Models](https://github.com/HowieHwong/TrustLLM), We introduce TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions.