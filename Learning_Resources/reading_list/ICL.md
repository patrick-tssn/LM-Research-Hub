# In Context Learning

Table of Content

- [Papers](#papers)
- [Reference](#reference)

## Papers

Survey

- [A Survey for In-context Learning](https://arxiv.org/abs/2301.00234), 2022.12

Reading List

| Title                                                                                                | Pub               | Preprint                                    | Supplementary                                             |
| ---------------------------------------------------------------------------------------------------- | ----------------- | ------------------------------------------- | --------------------------------------------------------- |
| Small Models are Valuable Plug-ins for Large Language Models                                         |                   | [2305.08848](https://arxiv.org/abs/2305.08848) | Microsoft                                                 |
| Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers | ACL 2023 Findings | [2212.10559](https://arxiv.org/abs/2212.10559) | [SuperICL](https://github.com/JetRunner/SuperICL), Microsoft |
| What learning algorithm is in-context learning? Investigations with linear models                    | ICLR 2023         | [2211.15661](https://arxiv.org/abs/2211.15661) | Google                                                    |
| Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?                          | EMNLP 2022        | [2202.12837](https://arxiv.org/abs/2202.12837) | gold label does not matter, UW                            |
| MetaICL: Learning to Learn In Context                                                                | NAACL 2021        | [2110.15943](https://arxiv.org/abs/2110.15943) | UW                                                        |
| Fantastically Ordered Prompts and Where to Find Them:Â Overcoming Few-Shot Prompt Order Sensitivity  | ACL 2022          | [2104.08786](https://arxiv.org/abs/2104.08786) | prompt order matters, UCL                                 |
| Language Models are Few-Shot Learners                                                                |                   | [2005.14165](https://arxiv.org/abs/2005.14165) | GPT3, propose In-Context Learning, OpenAI                 |

## Reference

- [ICL_PaperList](https://github.com/dqxiu/ICL_PaperList), Paper List for In-context Learning
- [LMaaS-Papers](https://github.com/txsun1997/LMaaS-Papers), Awesome papers on Language-Model-as-a-Service (LMaaS)
  > [black-box optimization](https://github.com/txsun1997/LMaaS-Papers#black-box-optimization)
  >
