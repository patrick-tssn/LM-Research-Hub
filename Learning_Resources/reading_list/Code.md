# Code

## Table of Contents

- [Papers](#papers)

## Papers

| Paper        | Base Language Model | Code                                      | Publication | Preprint | Affiliation |
| ------------ | ------------------- | ----------------------------------------- | ----------- | -------- | ----------- |
| OpenAI Codex |                     | [blog](https://openai.com/blog/openai-codex) |             |          | OpenAI      |

## Benchmarks and Datasets
- [SWE-bench](https://github.com/princeton-nlp/SWE-bench), SWE-bench is a benchmark for evaluating large language models on real world software issues collected from GitHub. Given a codebase and an issue, a language model is tasked with generating a patch that resolves the described problem.
- [xCodeEval](https://github.com/ntunlp/xCodeEval), We introduce xCodeEval, the largest executable multilingual multitask benchmark to date consisting of 25 M document-level coding examples from about 7.5K unique problems covering up to 17 programming languages with execution-level parallelism