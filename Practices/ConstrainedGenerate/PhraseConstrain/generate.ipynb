{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda3/envs/llama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is jack\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Define the vocabulary phrases you want to consider\n",
    "vocab_phrases = [\"hello world\", \"example test\", \"good morning\", \"my name is lucy\", \"my name is jack\"]\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = \"/data/prev_trained_models/Llama-3.2-1B\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Pre-encode the vocabulary phrases\n",
    "encoded_phrases = [torch.tensor(tokenizer.encode(phrase, add_special_tokens=False), device=device) for phrase in vocab_phrases]\n",
    "\n",
    "# Prepare input\n",
    "inputs = tokenizer(\"Hello, what's your name.\", return_tensors=\"pt\").to(device)\n",
    "generated_ids = inputs.input_ids\n",
    "max_new_tokens = 10\n",
    "start_indices = torch.zeros(len(encoded_phrases), dtype=torch.long, device=device)\n",
    "current_step = 0\n",
    "\n",
    "for _ in range(max_new_tokens):\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the logits of the last token\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "\n",
    "    # Create a mask for valid tokens at the current step\n",
    "    mask = (start_indices < current_step) | (start_indices >= torch.tensor([len(phrase) for phrase in encoded_phrases], device=device))\n",
    "    masked_logits = torch.full_like(logits, float('-inf'))\n",
    "    \n",
    "    # Fill masked_logits with the probabilities of the valid tokens\n",
    "    for i, phrase in enumerate(encoded_phrases):\n",
    "        if not mask[i]:\n",
    "            token_id = phrase[start_indices[i]]\n",
    "            masked_logits[0, token_id] = logits[0, token_id]\n",
    "\n",
    "    # Find the maximum probability token\n",
    "    max_prob, max_token_id = torch.max(masked_logits, dim=-1)\n",
    "\n",
    "    if max_prob.item() == float('-inf'):\n",
    "        break\n",
    "\n",
    "    # Update the generated sequence\n",
    "    next_token_id = max_token_id.unsqueeze(0)\n",
    "    generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "\n",
    "    # Update start indices for the chosen phrase\n",
    "    for i, phrase in enumerate(encoded_phrases):\n",
    "        if start_indices[i] < len(phrase) and phrase[start_indices[i]] == max_token_id.item():\n",
    "            start_indices[i] += 1\n",
    "    current_step += 1\n",
    "\n",
    "    # Update inputs for the next iteration\n",
    "    inputs = {\n",
    "        \"input_ids\": generated_ids,\n",
    "        \"attention_mask\": torch.ones(generated_ids.shape, device=device)\n",
    "    }\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text[len(\"Hello, what's your name.\"):])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
