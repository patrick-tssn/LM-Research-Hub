{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-xl')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-xl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get special ids in vocab\n",
    "print(tokenizer(['yes', 'no', 'Yes', 'No', 'A', 'B', 'C', 'D'])['input_ids'])\n",
    "# [[4273, 1], [150, 1], [2163, 1], [465, 1], [71, 1], [272, 1], [205, 1], [309, 1]]\n",
    "constrained_idx1 = [4273, 150]\n",
    "constrained_idx2 = [2163, 465]\n",
    "\n",
    "# get the scores of certain tokens\n",
    "inputs = tokenizer('Is the earth smaller then the basketball ?', return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs[\"input_ids\"],\n",
    "                         num_beams=1, \n",
    "                         return_dict_in_generate=True,\n",
    "                         output_scores=True)\n",
    "print(outputs.sequences)\n",
    "print(tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True))\n",
    "print(tokenizer.batch_decode(outputs.sequences))\n",
    "logits = outputs.scores[1] # skip the bos token\n",
    "constrained_logits = logits[:, constrained_idx1]\n",
    "print(torch.softmax(constrained_logits, dim=-1)) \n",
    "constrained_logits = logits[:, constrained_idx2]\n",
    "print(torch.softmax(constrained_logits, dim=-1)) \n",
    "constrained_logits = logits[:, constrained_idx1 + constrained_idx2]\n",
    "print(torch.softmax(constrained_logits, dim=-1)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InstructBLIP-flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, InstructBlipForConditionalGeneration\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "instructblip_processor = AutoProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\")\n",
    "instructblip_model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/instructblip-flan-t5-xl\").to(device)  # instructblip-flan-t5-xl\n",
    "instructblip_model.eval()\n",
    "\n",
    "constrained_index = instructblip_processor(text=['Yes', 'No', 'yes', 'no'])[\"input_ids\"]\n",
    "constrained_index = [x[0] for x in constrained_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad> no</s>']\n",
      "tensor([[0.4022, 0.5978]], device='cuda:0')\n",
      "tensor([[0.2335, 0.7665]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "image = Image.open('camel1.png')\n",
    "# image.thumbnail((640, 640), Image.Resampling.LANCZOS)\n",
    "instructblip_inputs = instructblip_processor(\n",
    "    text='Does \"there is any animal in the stocking\" correcly describe the image ?',\n",
    "    images=image.convert('RGB'),\n",
    "    return_tensors='pt',\n",
    ").to(device)\n",
    "outputs = instructblip_model.generate(**instructblip_inputs, \n",
    "                                            num_beams=1,\n",
    "                                            return_dict_in_generate=True,\n",
    "                                            output_scores=True)\n",
    "# responses = self.instructblip_processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "logits = outputs.scores[1]\n",
    "print(instructblip_processor.batch_decode(outputs.sequences))\n",
    "contrained_logits1 = logits[:, constrained_index[:2]]\n",
    "contrained_logits1 = torch.softmax(contrained_logits1, dim=-1)\n",
    "print(contrained_logits1) # probs of Yes No\n",
    "contrained_logits2 = logits[:, constrained_index[2:]]\n",
    "contrained_logits2 = torch.softmax(contrained_logits2, dim=-1)\n",
    "print(contrained_logits2) # probs of yes no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
